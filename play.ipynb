{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "bs, c, h = 2, 32, 64\n",
    "\n",
    "x = torch.rand((bs, c, h), dtype=torch.float)\n",
    "w1 = nn.Linear(h, h*2) # (h, c)\n",
    "\n",
    "y = w1(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[    -0., -65536., -65536., -65536., -65536., -65536., -65536.,\n",
      "           -65536., -65536., -65536.],\n",
      "          [    -0.,     -0., -65536., -65536., -65536., -65536., -65536.,\n",
      "           -65536., -65536., -65536.],\n",
      "          [    -0.,     -0.,     -0., -65536., -65536., -65536., -65536.,\n",
      "           -65536., -65536., -65536.],\n",
      "          [    -0.,     -0.,     -0.,     -0., -65536., -65536., -65536.,\n",
      "           -65536., -65536., -65536.],\n",
      "          [    -0.,     -0.,     -0.,     -0.,     -0., -65536., -65536.,\n",
      "           -65536., -65536., -65536.],\n",
      "          [    -0.,     -0.,     -0.,     -0.,     -0.,     -0., -65536.,\n",
      "           -65536., -65536., -65536.],\n",
      "          [    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "           -65536., -65536., -65536.],\n",
      "          [    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "               -0., -65536., -65536.],\n",
      "          [    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "               -0.,     -0., -65536.],\n",
      "          [    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "               -0.,     -0.,     -0.]]]], dtype=torch.bfloat16)\n",
      "tensor([[0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "seq_len = 10\n",
    "\n",
    "attention_mask = torch.tril(torch.ones((seq_len, seq_len), dtype = torch.bool)).view(1, 1, seq_len, seq_len)\n",
    "attention_mask = attention_mask.to(torch.bfloat16)\n",
    "attention_mask = (1.0 - attention_mask) * torch.finfo(torch.float16).min\n",
    "print(attention_mask)\n",
    "\n",
    "seqlen = 5\n",
    "start_pos = 2\n",
    "if seqlen > 1:\n",
    "    mask = torch.full((seqlen, seqlen), float(\"-inf\"))\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    # When performing key-value caching, we compute the attention scores\n",
    "    # only for the new sequence. Thus, the matrix of scores is of size\n",
    "    # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "    # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "    mask = torch.hstack(\n",
    "        [torch.zeros((seqlen, start_pos)), mask]\n",
    "    )\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.linear = nn.Linear(1024, 1024*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(1024*4, 1024)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without comppile: 0.721881628036499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gpt/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:124: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "witt comppile: 8.020703315734863\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "torch.set_default_device(\"cuda:0\")\n",
    "\n",
    "model = SampleModel()\n",
    "\n",
    "x = torch.randn(10, 1024, 1024)\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    y = model(x)\n",
    "print(\"without comppile:\", time.time() - start)\n",
    "\n",
    "model =  torch.compile(model)\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    y = model(x)\n",
    "print(\"witt comppile:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import torch\n",
    "\n",
    "from torch.cuda import Stream\n",
    "\n",
    "\n",
    "s = Stream()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "t1_cpu_pinned = torch.randn(1024**2 * 5, pin_memory=True)\n",
    "t2_cpu_paged = torch.randn(1024**2 * 5, pin_memory=False)\n",
    "t3_cuda = torch.randn(1024**2 * 5, device=\"cuda:0\")\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\", torch.cuda.current_device())\n",
    "\n",
    "\n",
    "# The function we want to profile\n",
    "def inner(pinned: bool, streamed: bool):\n",
    "    with torch.cuda.stream(s) if streamed else contextlib.nullcontext():\n",
    "        if pinned:\n",
    "            t1_cuda = t1_cpu_pinned.to(device, non_blocking=True)\n",
    "        else:\n",
    "            t2_cuda = t2_cpu_paged.to(device, non_blocking=True)\n",
    "        t_star_cuda_h2d_event = s.record_event()\n",
    "    # This operation can be executed during the CPU to GPU copy if and only if the tensor is pinned and the copy is\n",
    "    #  done in the other stream\n",
    "    t3_cuda_mul = t3_cuda * t3_cuda * t3_cuda\n",
    "    t3_cuda_h2d_event = torch.cuda.current_stream().record_event()\n",
    "    t_star_cuda_h2d_event.synchronize()\n",
    "    t3_cuda_h2d_event.synchronize()\n",
    "\n",
    "\n",
    "# Our profiler: profiles the `inner` function and stores the results in a .json file\n",
    "def benchmark_with_profiler(\n",
    "    pinned,\n",
    "    streamed,\n",
    ") -> None:\n",
    "    torch._C._profiler._set_cuda_sync_enabled_val(True)\n",
    "    wait, warmup, active = 1, 1, 2\n",
    "    num_steps = wait + warmup + active\n",
    "    rank = 0\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=wait, warmup=warmup, active=active, repeat=1, skip_first=1\n",
    "        ),\n",
    "    ) as prof:\n",
    "        for step_idx in range(1, num_steps + 1):\n",
    "            inner(streamed=streamed, pinned=pinned)\n",
    "            if rank is None or rank == 0:\n",
    "                prof.step()\n",
    "    prof.export_chrome_trace(f\"trace_streamed{int(streamed)}_pinned{int(pinned)}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-12-11 19:41:31 21742:21742 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-12-11 19:41:31 21742:21742 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-12-11 19:41:31 21742:21742 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "benchmark_with_profiler(streamed=True, pinned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_pretrained() missing 1 required positional argument: 'pretrained_model_name_or_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      3\u001b[0m tokenizer_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/duyong/model-zoos/meta-llama/Meta-Llama-3.1-8B-Instruct-oooooooold/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token  \u001b[38;5;66;03m# Most LLMs don't have a pad token by default\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m      7\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1, 2, 3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA, B, C, D, E\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: from_pretrained() missing 1 required positional argument: 'pretrained_model_name_or_path'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_path: str = \"/home/duyong/model-zoos/meta-llama/Meta-Llama-3.1-8B-Instruct-oooooooold/\",\n",
    "tokenizer = AutoTokenizer.from_pretrained(model=tokenizer_path, padding_side=\"left\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tok_embeddings.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.attention_norm.weight', 'layers.1.ffn_norm.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wv.weight', 'layers.2.attention.wo.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.attention_norm.weight', 'layers.2.ffn_norm.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wv.weight', 'layers.3.attention.wo.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.attention_norm.weight', 'layers.3.ffn_norm.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wv.weight', 'layers.4.attention.wo.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.attention_norm.weight', 'layers.4.ffn_norm.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wv.weight', 'layers.5.attention.wo.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.attention_norm.weight', 'layers.5.ffn_norm.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wv.weight', 'layers.6.attention.wo.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.attention_norm.weight', 'layers.6.ffn_norm.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wv.weight', 'layers.7.attention.wo.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.attention_norm.weight', 'layers.7.ffn_norm.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wv.weight', 'layers.8.attention.wo.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.attention_norm.weight', 'layers.8.ffn_norm.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wv.weight', 'layers.9.attention.wo.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.attention_norm.weight', 'layers.9.ffn_norm.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wv.weight', 'layers.10.attention.wo.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.attention_norm.weight', 'layers.10.ffn_norm.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wv.weight', 'layers.11.attention.wo.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.attention_norm.weight', 'layers.11.ffn_norm.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wv.weight', 'layers.12.attention.wo.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.attention_norm.weight', 'layers.12.ffn_norm.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wv.weight', 'layers.13.attention.wo.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.attention_norm.weight', 'layers.13.ffn_norm.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wv.weight', 'layers.14.attention.wo.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.attention_norm.weight', 'layers.14.ffn_norm.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wv.weight', 'layers.15.attention.wo.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.attention_norm.weight', 'layers.15.ffn_norm.weight', 'layers.16.attention.wq.weight', 'layers.16.attention.wk.weight', 'layers.16.attention.wv.weight', 'layers.16.attention.wo.weight', 'layers.16.feed_forward.w1.weight', 'layers.16.feed_forward.w3.weight', 'layers.16.feed_forward.w2.weight', 'layers.16.attention_norm.weight', 'layers.16.ffn_norm.weight', 'layers.17.attention.wq.weight', 'layers.17.attention.wk.weight', 'layers.17.attention.wv.weight', 'layers.17.attention.wo.weight', 'layers.17.feed_forward.w1.weight', 'layers.17.feed_forward.w3.weight', 'layers.17.feed_forward.w2.weight', 'layers.17.attention_norm.weight', 'layers.17.ffn_norm.weight', 'layers.18.attention.wq.weight', 'layers.18.attention.wk.weight', 'layers.18.attention.wv.weight', 'layers.18.attention.wo.weight', 'layers.18.feed_forward.w1.weight', 'layers.18.feed_forward.w3.weight', 'layers.18.feed_forward.w2.weight', 'layers.18.attention_norm.weight', 'layers.18.ffn_norm.weight', 'layers.19.attention.wq.weight', 'layers.19.attention.wk.weight', 'layers.19.attention.wv.weight', 'layers.19.attention.wo.weight', 'layers.19.feed_forward.w1.weight', 'layers.19.feed_forward.w3.weight', 'layers.19.feed_forward.w2.weight', 'layers.19.attention_norm.weight', 'layers.19.ffn_norm.weight', 'layers.20.attention.wq.weight', 'layers.20.attention.wk.weight', 'layers.20.attention.wv.weight', 'layers.20.attention.wo.weight', 'layers.20.feed_forward.w1.weight', 'layers.20.feed_forward.w3.weight', 'layers.20.feed_forward.w2.weight', 'layers.20.attention_norm.weight', 'layers.20.ffn_norm.weight', 'layers.21.attention.wq.weight', 'layers.21.attention.wk.weight', 'layers.21.attention.wv.weight', 'layers.21.attention.wo.weight', 'layers.21.feed_forward.w1.weight', 'layers.21.feed_forward.w3.weight', 'layers.21.feed_forward.w2.weight', 'layers.21.attention_norm.weight', 'layers.21.ffn_norm.weight', 'layers.22.attention.wq.weight', 'layers.22.attention.wk.weight', 'layers.22.attention.wv.weight', 'layers.22.attention.wo.weight', 'layers.22.feed_forward.w1.weight', 'layers.22.feed_forward.w3.weight', 'layers.22.feed_forward.w2.weight', 'layers.22.attention_norm.weight', 'layers.22.ffn_norm.weight', 'layers.23.attention.wq.weight', 'layers.23.attention.wk.weight', 'layers.23.attention.wv.weight', 'layers.23.attention.wo.weight', 'layers.23.feed_forward.w1.weight', 'layers.23.feed_forward.w3.weight', 'layers.23.feed_forward.w2.weight', 'layers.23.attention_norm.weight', 'layers.23.ffn_norm.weight', 'layers.24.attention.wq.weight', 'layers.24.attention.wk.weight', 'layers.24.attention.wv.weight', 'layers.24.attention.wo.weight', 'layers.24.feed_forward.w1.weight', 'layers.24.feed_forward.w3.weight', 'layers.24.feed_forward.w2.weight', 'layers.24.attention_norm.weight', 'layers.24.ffn_norm.weight', 'layers.25.attention.wq.weight', 'layers.25.attention.wk.weight', 'layers.25.attention.wv.weight', 'layers.25.attention.wo.weight', 'layers.25.feed_forward.w1.weight', 'layers.25.feed_forward.w3.weight', 'layers.25.feed_forward.w2.weight', 'layers.25.attention_norm.weight', 'layers.25.ffn_norm.weight', 'layers.26.attention.wq.weight', 'layers.26.attention.wk.weight', 'layers.26.attention.wv.weight', 'layers.26.attention.wo.weight', 'layers.26.feed_forward.w1.weight', 'layers.26.feed_forward.w3.weight', 'layers.26.feed_forward.w2.weight', 'layers.26.attention_norm.weight', 'layers.26.ffn_norm.weight', 'layers.27.attention.wq.weight', 'layers.27.attention.wk.weight', 'layers.27.attention.wv.weight', 'layers.27.attention.wo.weight', 'layers.27.feed_forward.w1.weight', 'layers.27.feed_forward.w3.weight', 'layers.27.feed_forward.w2.weight', 'layers.27.attention_norm.weight', 'layers.27.ffn_norm.weight', 'layers.28.attention.wq.weight', 'layers.28.attention.wk.weight', 'layers.28.attention.wv.weight', 'layers.28.attention.wo.weight', 'layers.28.feed_forward.w1.weight', 'layers.28.feed_forward.w3.weight', 'layers.28.feed_forward.w2.weight', 'layers.28.attention_norm.weight', 'layers.28.ffn_norm.weight', 'layers.29.attention.wq.weight', 'layers.29.attention.wk.weight', 'layers.29.attention.wv.weight', 'layers.29.attention.wo.weight', 'layers.29.feed_forward.w1.weight', 'layers.29.feed_forward.w3.weight', 'layers.29.feed_forward.w2.weight', 'layers.29.attention_norm.weight', 'layers.29.ffn_norm.weight', 'layers.30.attention.wq.weight', 'layers.30.attention.wk.weight', 'layers.30.attention.wv.weight', 'layers.30.attention.wo.weight', 'layers.30.feed_forward.w1.weight', 'layers.30.feed_forward.w3.weight', 'layers.30.feed_forward.w2.weight', 'layers.30.attention_norm.weight', 'layers.30.ffn_norm.weight', 'layers.31.attention.wq.weight', 'layers.31.attention.wk.weight', 'layers.31.attention.wv.weight', 'layers.31.attention.wo.weight', 'layers.31.feed_forward.w1.weight', 'layers.31.feed_forward.w3.weight', 'layers.31.feed_forward.w2.weight', 'layers.31.attention_norm.weight', 'layers.31.ffn_norm.weight', 'norm.weight', 'output.weight'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "bin_file = '/home/duyong/model-zoos/meta-llama/Meta-Llama-3.1-8B-Instruct-oooooooold/original/consolidated.00.pth'\n",
    "state = torch.load(bin_file, map_location='cpu')\n",
    "state.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight: torch.Size([128256, 4096])\n",
      "layers.0.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.0.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.0.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.0.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.0.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.0.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.0.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.0.attention_norm.weight: torch.Size([4096])\n",
      "layers.0.ffn_norm.weight: torch.Size([4096])\n",
      "layers.1.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.1.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.1.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.1.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.1.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.1.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.1.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.1.attention_norm.weight: torch.Size([4096])\n",
      "layers.1.ffn_norm.weight: torch.Size([4096])\n",
      "layers.2.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.2.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.2.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.2.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.2.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.2.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.2.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.2.attention_norm.weight: torch.Size([4096])\n",
      "layers.2.ffn_norm.weight: torch.Size([4096])\n",
      "layers.3.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.3.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.3.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.3.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.3.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.3.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.3.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.3.attention_norm.weight: torch.Size([4096])\n",
      "layers.3.ffn_norm.weight: torch.Size([4096])\n",
      "layers.4.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.4.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.4.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.4.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.4.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.4.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.4.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.4.attention_norm.weight: torch.Size([4096])\n",
      "layers.4.ffn_norm.weight: torch.Size([4096])\n",
      "layers.5.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.5.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.5.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.5.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.5.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.5.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.5.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.5.attention_norm.weight: torch.Size([4096])\n",
      "layers.5.ffn_norm.weight: torch.Size([4096])\n",
      "layers.6.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.6.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.6.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.6.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.6.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.6.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.6.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.6.attention_norm.weight: torch.Size([4096])\n",
      "layers.6.ffn_norm.weight: torch.Size([4096])\n",
      "layers.7.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.7.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.7.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.7.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.7.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.7.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.7.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.7.attention_norm.weight: torch.Size([4096])\n",
      "layers.7.ffn_norm.weight: torch.Size([4096])\n",
      "layers.8.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.8.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.8.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.8.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.8.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.8.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.8.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.8.attention_norm.weight: torch.Size([4096])\n",
      "layers.8.ffn_norm.weight: torch.Size([4096])\n",
      "layers.9.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.9.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.9.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.9.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.9.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.9.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.9.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.9.attention_norm.weight: torch.Size([4096])\n",
      "layers.9.ffn_norm.weight: torch.Size([4096])\n",
      "layers.10.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.10.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.10.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.10.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.10.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.10.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.10.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.10.attention_norm.weight: torch.Size([4096])\n",
      "layers.10.ffn_norm.weight: torch.Size([4096])\n",
      "layers.11.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.11.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.11.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.11.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.11.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.11.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.11.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.11.attention_norm.weight: torch.Size([4096])\n",
      "layers.11.ffn_norm.weight: torch.Size([4096])\n",
      "layers.12.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.12.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.12.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.12.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.12.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.12.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.12.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.12.attention_norm.weight: torch.Size([4096])\n",
      "layers.12.ffn_norm.weight: torch.Size([4096])\n",
      "layers.13.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.13.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.13.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.13.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.13.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.13.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.13.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.13.attention_norm.weight: torch.Size([4096])\n",
      "layers.13.ffn_norm.weight: torch.Size([4096])\n",
      "layers.14.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.14.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.14.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.14.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.14.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.14.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.14.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.14.attention_norm.weight: torch.Size([4096])\n",
      "layers.14.ffn_norm.weight: torch.Size([4096])\n",
      "layers.15.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.15.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.15.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.15.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.15.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.15.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.15.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.15.attention_norm.weight: torch.Size([4096])\n",
      "layers.15.ffn_norm.weight: torch.Size([4096])\n",
      "layers.16.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.16.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.16.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.16.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.16.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.16.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.16.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.16.attention_norm.weight: torch.Size([4096])\n",
      "layers.16.ffn_norm.weight: torch.Size([4096])\n",
      "layers.17.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.17.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.17.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.17.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.17.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.17.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.17.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.17.attention_norm.weight: torch.Size([4096])\n",
      "layers.17.ffn_norm.weight: torch.Size([4096])\n",
      "layers.18.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.18.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.18.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.18.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.18.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.18.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.18.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.18.attention_norm.weight: torch.Size([4096])\n",
      "layers.18.ffn_norm.weight: torch.Size([4096])\n",
      "layers.19.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.19.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.19.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.19.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.19.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.19.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.19.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.19.attention_norm.weight: torch.Size([4096])\n",
      "layers.19.ffn_norm.weight: torch.Size([4096])\n",
      "layers.20.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.20.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.20.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.20.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.20.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.20.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.20.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.20.attention_norm.weight: torch.Size([4096])\n",
      "layers.20.ffn_norm.weight: torch.Size([4096])\n",
      "layers.21.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.21.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.21.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.21.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.21.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.21.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.21.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.21.attention_norm.weight: torch.Size([4096])\n",
      "layers.21.ffn_norm.weight: torch.Size([4096])\n",
      "layers.22.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.22.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.22.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.22.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.22.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.22.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.22.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.22.attention_norm.weight: torch.Size([4096])\n",
      "layers.22.ffn_norm.weight: torch.Size([4096])\n",
      "layers.23.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.23.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.23.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.23.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.23.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.23.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.23.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.23.attention_norm.weight: torch.Size([4096])\n",
      "layers.23.ffn_norm.weight: torch.Size([4096])\n",
      "layers.24.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.24.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.24.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.24.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.24.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.24.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.24.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.24.attention_norm.weight: torch.Size([4096])\n",
      "layers.24.ffn_norm.weight: torch.Size([4096])\n",
      "layers.25.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.25.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.25.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.25.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.25.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.25.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.25.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.25.attention_norm.weight: torch.Size([4096])\n",
      "layers.25.ffn_norm.weight: torch.Size([4096])\n",
      "layers.26.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.26.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.26.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.26.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.26.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.26.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.26.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.26.attention_norm.weight: torch.Size([4096])\n",
      "layers.26.ffn_norm.weight: torch.Size([4096])\n",
      "layers.27.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.27.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.27.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.27.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.27.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.27.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.27.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.27.attention_norm.weight: torch.Size([4096])\n",
      "layers.27.ffn_norm.weight: torch.Size([4096])\n",
      "layers.28.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.28.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.28.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.28.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.28.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.28.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.28.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.28.attention_norm.weight: torch.Size([4096])\n",
      "layers.28.ffn_norm.weight: torch.Size([4096])\n",
      "layers.29.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.29.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.29.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.29.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.29.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.29.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.29.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.29.attention_norm.weight: torch.Size([4096])\n",
      "layers.29.ffn_norm.weight: torch.Size([4096])\n",
      "layers.30.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.30.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.30.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.30.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.30.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.30.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.30.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.30.attention_norm.weight: torch.Size([4096])\n",
      "layers.30.ffn_norm.weight: torch.Size([4096])\n",
      "layers.31.attention.wq.weight: torch.Size([4096, 4096])\n",
      "layers.31.attention.wk.weight: torch.Size([1024, 4096])\n",
      "layers.31.attention.wv.weight: torch.Size([1024, 4096])\n",
      "layers.31.attention.wo.weight: torch.Size([4096, 4096])\n",
      "layers.31.feed_forward.w1.weight: torch.Size([14336, 4096])\n",
      "layers.31.feed_forward.w2.weight: torch.Size([4096, 14336])\n",
      "layers.31.feed_forward.w3.weight: torch.Size([14336, 4096])\n",
      "layers.31.attention_norm.weight: torch.Size([4096])\n",
      "layers.31.ffn_norm.weight: torch.Size([4096])\n",
      "norm.weight: torch.Size([4096])\n",
      "output.weight: torch.Size([128256, 4096])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from zllm.worker.llama31 import Transformer, ModelArgs\n",
    "\n",
    "ckpt_dir = '/home/duyong/model-zoos/meta-llama/Meta-Llama-3.1-8B-Instruct-oooooooold/original'\n",
    "with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "    params = json.loads(f.read())\n",
    "\n",
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=4096,\n",
    "    max_batch_size=16,\n",
    "    flash=False,\n",
    "    paged=False,\n",
    "    **params,\n",
    ")\n",
    "model = Transformer(model_args)\n",
    "state_dict =  model.state_dict()\n",
    "\n",
    "row_parallel_ = [\"tok_embeddings\"]\n",
    "col_parallel = [\"ln_f.weight\", \"ln_f.bias\"]\n",
    "\n",
    "for key, value in state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5607,  0.5485, -0.6634, -0.4783, -0.5522,  0.0477, -0.0076,\n",
      "          -1.6286],\n",
      "         [ 1.4370,  0.7416,  0.1552,  1.0217,  0.2171, -0.1649, -0.0697,\n",
      "          -0.0686]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1, 2, 8)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5607+0.5485j, -0.6634-0.4783j, -0.5522+0.0477j, -0.0076-1.6286j],\n",
      "         [ 1.4370+0.7416j,  0.1552+1.0217j,  0.2171-0.1649j, -0.0697-0.0686j]]])\n",
      "tensor([[[ 0.5607,  0.5485, -0.6634, -0.4783, -0.5522,  0.0477, -0.0076,\n",
      "          -1.6286],\n",
      "         [ 1.4370,  0.7416,  0.1552,  1.0217,  0.2171, -0.1649, -0.0697,\n",
      "          -0.0686]]])\n"
     ]
    }
   ],
   "source": [
    "x_c = torch.view_as_complex(x.reshape(1, 2, -1, 2))\n",
    "print(x_c)\n",
    "\n",
    "x_o = torch.view_as_real(x_c).flatten(2)\n",
    "print(x_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2500e-01, 1.2500e-05, 1.2500e-09, 1.2500e-13])\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.2500e-01, 1.2500e-05, 1.2500e-09, 1.2500e-13],\n",
      "        [2.5000e-01, 2.5000e-05, 2.5000e-09, 2.5000e-13],\n",
      "        [3.7500e-01, 3.7500e-05, 3.7500e-09, 3.7500e-13]])\n",
      "tensor([[1.0000+0.0000e+00j, 1.0000+0.0000e+00j, 1.0000+0.0000e+00j,\n",
      "         1.0000+0.0000e+00j],\n",
      "        [0.9922+1.2467e-01j, 1.0000+1.2500e-05j, 1.0000+1.2500e-09j,\n",
      "         1.0000+1.2500e-13j],\n",
      "        [0.9689+2.4740e-01j, 1.0000+2.5000e-05j, 1.0000+2.5000e-09j,\n",
      "         1.0000+2.5000e-13j],\n",
      "        [0.9305+3.6627e-01j, 1.0000+3.7500e-05j, 1.0000+3.7500e-09j,\n",
      "         1.0000+3.7500e-13j]])\n"
     ]
    }
   ],
   "source": [
    "dim = 8\n",
    "max_len = 4\n",
    "theta = 100.0\n",
    "\n",
    "freqs = (1.0 / (theta**torch.arange(0, dim, 2)[:(dim//2)]).float() / dim)\n",
    "print(freqs)\n",
    "\n",
    "t = torch.arange(max_len, dtype=torch.float32)\n",
    "print(t)\n",
    "\n",
    "freqs = torch.outer(t, freqs)\n",
    "print(freqs)\n",
    "\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "print(freqs_cis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0000e+00+0.0000e+00j, -8.7423e-08+2.0000e+00j,\n",
      "        -3.0000e+00-2.6227e-07j])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义径向距离和角度\n",
    "r = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float)\n",
    "theta = torch.tensor([0.0, torch.pi / 2, torch.pi], dtype=torch.float)\n",
    "\n",
    "# 将极坐标转换为笛卡尔坐标\n",
    "cartesian_coords = torch.polar(r, theta)\n",
    "\n",
    "print(cartesian_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis_ntk(dim: int, end: int, theta: float = 10000.0, alpha: int= 16):\n",
    "    theta = theta * alpha ** (dim / (dim-2))\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)  # real\n",
    "    freqs_sin = torch.sin(freqs)  # imaginary\n",
    "    return freqs_cos, freqs_sin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
